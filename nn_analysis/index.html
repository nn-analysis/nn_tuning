<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>nn_analysis API documentation</title>
<meta name="description" content="[comment]: &lt;&gt;
(- Something about what this package is)
[comment]: &lt;&gt;
(- Something about the license)
[comment]: &lt;&gt;
(- Some links to the GitHub …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>nn_analysis</code></h1>
</header>
<section id="section-intro">
<p>Analyse neural networks for feature tuning.</p>
<h2 id="installing-and-configuring-nn_analysis">Installing and configuring nn_analysis</h2>
<p>Install nn_analysis using the following <code>pip</code> command. This requires python &gt; 3.6.</p>
<pre><code>$ pip install nn_analysis
</code></pre>
<p>If you wish to use the plotting functions in any of the classes, you need to install Matplotlib. To do so you can use the following command:</p>
<pre><code>$ pip install matplotlib
</code></pre>
<p>Depending on the type of neural network you want to analyse you will need to install PyTorch or TensorFlow.
The required packages per network are listed in the table below.
When importing the network class, if the required pacakages are not installed, the error should also let you know what packages it expects.</p>
<table>
<thead>
<tr>
<th>Network</th>
<th>Package</th>
<th>Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>AlexNet</td>
<td>pytorch<br>pytorchvision</td>
<td>latest</td>
</tr>
<tr>
<td>PredNet</td>
<td>tensorflow<br>python</td>
<td>&lt; 2<br>3.6.*</td>
</tr>
</tbody>
</table>
<p>To install PyTorch use the following <code>pip</code> command:</p>
<pre><code>$ pip install pytorch
</code></pre>
<p>To install tensorflow use the following <code>pip</code> command:</p>
<pre><code>$ pip install tensorflow.
</code></pre>
<p>The PredNet specific environment can also be installed by using the prednet variant of the package in <code>pip</code> using the following <code>pip</code> command.</p>
<pre><code>$ pip install nn_analysis-prednet
</code></pre>
<h2 id="getting-started">Getting started</h2>
<p>To get started first import the package and the inputs and networks you want to use.</p>
<pre><code>from nn_analysis import *
from nn_analysis.networks.prednet import Prednet
</code></pre>
<p>Define a table and a database to store the results in and initialise the storage manager.</p>
<pre><code>table = 'activations_table'
database = Database("/path/to/database/folder/")
storage_manager = StorageManager(database)
</code></pre>
<h3 id="getting-the-activations">Getting the activations</h3>
<p>Now we need to set up an input manager. The input will provide stimuli for the network using the input generator. In order to make a new input generator please see (link to that part of the documentation).
Here we will use the example of the build in <code>PRFInputManager</code>.</p>
<p>The input manager requires an input shape. This is the shape of the first layer in the network you want to use.
It is also possible to set a verbose flag for the input generator. By default, this flag is False.</p>
<pre><code>verbose = True
input_shape = (1,3,128,160)
prf_input_generator = PRFInputGenerator(1, 'prf_input', storage_manager, verbose=verbose)
prf_input_manager = InputManager(TableSet('prf_input', database), input_shape, prf_input_generator)
</code></pre>
<p>We then initialise the network. In this case I am using the prednet network as an example. The <code>json_file</code> and <code>weight_file</code> variables are strings with the location of those files.
The presentation variable determines the way stimuli are presented to the network. This way it is possible to get intermediates from the recurrent process rather than just the final result.
By default the network uses an iterative presentation and takes the mean from all the recorded iterative activations as an output.</p>
<pre><code>network = Prednet(json_file, weights_file, presentation='iterative')
</code></pre>
<p>We then define an output manager using the network, storage manager, and the input manager.</p>
<pre><code>output_manager = OutputManager(network, storage_manager, prf_nd_input_manager)
</code></pre>
<p>Now we can present the stimuli to the network batch wise. This step can take some time.
The resume parameter makes the network resume in case the program is halted intermediately.</p>
<pre><code>output_manager.run(table, batch_size=20, resume=True, verbose=True)
</code></pre>
<h3 id="fitting-activations-to-a-tuning-function">Fitting activations to a tuning function</h3>
<p>First open the table containing the activations by using the storage manager.</p>
<pre><code>responses_table_set = storage_manager.open_table(table)
</code></pre>
<p>Next initialise the fitting manager</p>
<pre><code>fitting_manager = FittingManager(storage_manager)
</code></pre>
<p>Now we need some variables that are required for the fitting manager to work.</p>
<p>The <code>stim_x</code>, <code>stim_y</code>, and <code>stimulus</code> variables are used in the fitting procedure to generate a prediction from the function parameters it is testing. <code>stim_x</code> and <code>stim_y</code> both contain the feature representation of the thing you were trying to present.
So in the case of position data <code>stim_x</code> and <code>stim_y</code> are of size 128*160 and represent every point in the input image for image position data.
If the data you are testing is one dimensional, you can initialise the <code>stim_y</code> to a list of zeros of the same size as <code>stim_x</code>.
The <code>stimulus</code> variable represents which features we stimulated in each stimulus.
So the size of the <code>stimulus</code> variable is always the amount of stimuli that were presented x the size of <code>stim_x</code></p>
<pre><code>stim_x, stim_y = fitting_manager.get_identity_stim_variables(*shape)
stimulus = prf_input_generator.get_stimulus(shape)
</code></pre>
<p>Next we need to initialise the parameter set. This is the set of parameters that will be tested by the fitting manager.
To do this it is possible to use the <code>init_parameter_set</code> function from the <code>FittingManager</code>.
This function requires a step size for each function parameter (x, y, and sigma) as well as the maximum value for each of those.
Finally, the function has an optional parameter for if the sigma should be linearised. This is useful when you want to use a logarithmic tuning function.
In this case we don't, so we left it False.</p>
<pre><code>shape = (128, 160)
candidate_function_parameters = FittingManager.init_parameter_set((x_step, y_step, sigma_step), (*shape, max_sigma),
                                                                  linearise_s=False)
</code></pre>
<p>Next, we need to pick a table name to store the results in.</p>
<pre><code>fitting_results_table = f"{table}_fitting_results"
</code></pre>
<p>Finally, we can run the actual fitting procedure. By default, this function splits the calculation of the results into separate parts to not overload the memory or CPU.
The resulting <code>TableSet</code> is returned by the function. </p>
<p>By default, this function uses a gaussian tuning function. To use a different tuning function you can provide the <code>prediction_function</code> parameter.
This parameter is a string that is evaluated in the function. In this code you have the <code>stim_x</code> and <code>stim_y</code> variable as well as the <code>x</code>, <code>y</code>, and <code>sigma</code> for the function from the function parameter set.</p>
<pre><code>results_tbl_set = fitting_manager.fit_response_function_on_table_set(responses_table_set, fitting_results_table,
                                                                     stim_x, stim_y, candidate_function_parameters,
                                                                     stimulus=stimulus,
                                                                     verbose=True,
                                                                     dtype=np.dtype('float16'))
</code></pre>
<p>Since the <code>results_tbl_set</code> contains all results, we need to still calculate which function had the best fit for each node in the network.
For this the <code>FittingManager</code> has a <code>calculate_best_fits</code> function that takes the <code>candidate_function_parameters</code> and the <code>results_tbl_set</code> and stores the best fits in a new table.</p>
<pre><code>best_fit_results_tbl = fitting_manager.calculate_best_fits(results_tbl_set, candidate_function_parameters, table+'_best')
</code></pre>
<h3 id="plot-the-results">Plot the results</h3>
<p>You can choose many types of plots depending on the need in your project.
Here we give an example of a plot that might be more commonly useful as well as an explanation of how to access the relevant data for your plots.</p>
<p>Before we start plotting, it is good to understand how the results from the previous step look.
The best fits <code>TableSet</code> in the final step of the fitting procedure contains four rows.
The rows contain the goodness of fit, preferred x position, preferred y position, and the preferred σ respectively.
So, in order to retrieve the data for our plot we have to select the row with the type of data we want, and the column with the nodes in the network.</p>
<p>Getting the part of the network that you want to look at is easy thanks to the <code>get_subtable</code> function in the <code>TableSet</code> class.
In order to select just the first layer in a network all you need to do is <code>tableset.get_subtable(0)</code>.
The returned value is a <code>Table</code> or <code>TableSet</code> that both support slicing in the same way, so that any subsequent functions can be called unaltered.
For documentation about slicing in the <code>Table</code> or <code>TableSet</code> please see the documentation for those classes.</p>
<p>Now you are probably wondering: How does this look in practice?
Below is a bit of code that plots, for each layer, the field of vision (σ in the case of positional data).</p>
<pre><code>import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

for layer_subtable in best_fit_results_tbl.subtables:
    goodness_of_fits, pref_x, pref_y, pref_s = best_fit_results_tbl.get_subtable(layer_subtable)[:]
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1, projection='scatter_density')
    white_viridis = LinearSegmentedColormap.from_list('white_viridis', [
        (0, '#ffffff'),
        (1e-20, '#440053'),
        (0.2, '#404388'),
        (0.4, '#2a788e'),
        (0.6, '#21a784'),
        (0.8, '#78d151'),
        (1, '#fde624'),
    ], N=256)
    density = ax.scatter_density(pref_s, goodness_of_fits, cmap=white_viridis)
    fig.colorbar(density, label='Number of neurons per pixel')
    ax.set_ylabel('Goodness of Fit')
    ax.set_xlabel('Field of vision')
    Plot.show(plt)
</code></pre>
<p>As you can see, we go through all the subtables in the main <code>TableSet</code>. In PredNet these correspond to the layers.
We then get the best fits for that layer using the <code>get_subtable</code> function.
Finally, we plot the GoF against the σ value using a matplotlib scatter plot.</p>
<h2 id="adding-new-neural-networks-to-the-code-analysis-system">Adding new neural networks to the code analysis system</h2>
<p>In order to extend the code analysis system to new neural networks a new class has to be created for that network in the networks sub package. This class has to extend the network class from that same sub package. </p>
<p>The new class has to implement a run function. The run function should run a batch of inputs, given in the input variable, through the model, record activations, and return those activations in the form of a nested tuple of np arrays along with a names dictionary that gives names to each of the items in the nested tuple.</p>
<p>Variables specific to the network can be added to the initialisation function of the class.</p>
<p>In practice, for most hierarchical networks, this all means setting up a model in the <code>__init__</code> function and running the input through that model in the <code>run</code> function. For pre-trained hierarchical PyTorch and TensorFlow/Keras models this means that it is possible to use a fairly standardised approach to building a new network class since recording activations has standardised functions.</p>
<h3 id="pytorch-models">PyTorch models</h3>
<p>For PyTorch models it is possible to load the model using the functions in the submodules in <code>torchvision.models</code> and then registering hooks for the layers in that model using the following function. This function also fills a labels variable that you can use as a names variable when returning output in the <code>run</code> function. Run this function after setting up the model in the <code>__init__</code> function. For an example of this method in use please see the AlexNet class.</p>
<pre><code>def __register_hooks(self):
    &quot;&quot;&quot;
    Function that registers hooks to save results from the network model in the run function.
    &quot;&quot;&quot;
    def hook_wrapper(name: str):
        def hook(_, __, output):
            self.__raw_output[name] = output.detach().numpy()

        return hook
    for submodel_name, submodel in self.model.named_modules():
        if type(submodel) is not AlexNetModel and type(submodel) is not nn.Sequential:
            self.labels[submodel_name] = submodel_name
            submodel.register_forward_hook(hook_wrapper(submodel_name))
</code></pre>
<p>Note that this does not work well for recurrent models. There you would need to build your own implementation specific to that network.</p>
<h3 id="tensorflow-models">TensorFlow models</h3>
<p>For TensorFlow you will need a slightly different function but with much of the same idea. In the case of TensorFlow, the way to do this generally is to create a second model with the weights of the previous network. This new model has layers that are enclosed in a new type of layer that is accessible by hooks in a similar way to PyTorch models. The enclosed layer is shown below.</p>
<pre><code>import tensorflow as tf
from typing import List, Callable, Optional

class LayerWithHooks(tf.keras.layers.Layer):
  def __init__(
      self, 
      layer: tf.keras.layers.Layer,
      hooks: List[Callable[[tf.Tensor, tf.Tensor], Optional[tf.Tensor]]] = None):
    super().__init__()
    self._layer = layer
    self._hooks = hooks or []

  def call(self, input: tf.Tensor) -&gt; tf.Tensor:
    output = self._layer(input)
    for hook in self._hooks:
      hook_result = hook(input, output)
      if hook_result is not None:
        output = hook_result
    return output

  def register_hook(
      self, 
      hook: Callable[[tf.Tensor, tf.Tensor], Optional[tf.Tensor]]) -&gt; None:
    self._hooks.append(hook)`
</code></pre>
<p>The method that registers those hooks is slightly more difficult than the one from PyTorch. An example of a method registering hooks is shown in the code below. In contrast to the default way of doing this in PyTorch, TensorFlow cannot automatically register hooks to each layer. Rather, this code has to be altered based on the network to add each layer in that network to the second model. The second model is the model that should eventually be called in the <code>run</code> function.</p>
<pre><code>def __register_hooks(self):
    &quot;&quot;&quot;
    Function that registers hooks to save results from the network model in the run function.
    &quot;&quot;&quot;
    self.__raw_output = {}
    def hook_wrapper(name: str):
        def hook(_, output):
            self.__raw_output[name] = tf.identity(output).numpy()

        return hook

    # This second model should have the same layers and structure that the original model in self.model has.
    # For each layer you take a copy of the original layer that you add to the new model wrapped in a LayerWithHooks layer.
    # The layer with hooks should have the hook_wrapper that we defined above.
    # The run function should use the self.model2 model to run the network.
    self.model2 = Sequential()
    self.model2.add(LayerWithHooks(Dense(20, 64, weights=model.layers[0].get_weights()), [hook_wrapper('First dense layer')]))
    self.model2.add(Activation('tanh'))
</code></pre>
<h2 id="implement-your-own-stimulus-set">Implement your own stimulus set</h2>
<p>When running your own experiments, you will likely want to design a stimulus set tailored to that experiment.
To do so, you have to implement the <code>InputGenerator</code> class.</p>
<p>The <code>InputGenerator</code> has only one function you have to implement: <code>generate()</code>.
<code>generate()</code> has one parameter <code>shape</code>. This parameter is the shape of the eventual complete output.
Any other variables you might want to use in your stimulus have to be in the <code>__init__()</code> method.
<code>generate()</code> does not return the output but only saves the stimuli in a <code>Table</code>/<code>TableSet</code>.
How you implement the generation of the stimuli is entirely up to you. </p>
<p>For two-dimensional stimuli you can choose to implement the class <code>TwoDInputGenerator</code>.
This class has a pre-build function that automatically fills any other dimensions that a network might have such as, in the case of PredNet, a time dimension.
In order to implement it you have to implement the <code>_get_2d()</code> function. This function has two parameters: a two-dimensional shape, and an index.
You can use the two-dimensional shape to determine the shape of the output and the index to determine what to output at this point.
You still have to implement the <code>generate()</code> function from the <code>InputGenerator</code> class.
In your implementation of this function you can use the <code>_generate_row()</code> function from the <code>TwoDInputGenerator</code> with you own indexing system.
For an example of how to do this you can look at the <code>PRFInputGenerator</code>.</p>
<pre><code>def generate(self, shape: tuple) -&gt; Union[Table, TableSet]:
    """
    Generates all input and saves the input to a table

    Args:
        shape: (tuple) The expected shape of the input

    Returns:
        &lt;code&gt;Table&lt;/code&gt; or &lt;code&gt;TableSet&lt;/code&gt; containing the stimuli
    """
    tbl = None
    size_x = shape[-1]
    size_y = shape[-2]
    for i in tqdm(range(0, size_x + size_y + 2, self.__stride), leave=False, disable=(not self.__verbose)):
        tbl = self.__storage_manager.save_result_table_set((self._generate_row(shape, i)[np.newaxis, ...],),
                                                           self.__table, {self.__table: self.__table},
                                                           append_rows=True)
    return tbl
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
.. include:: ./introduction.md
.. include:: ./getting_started.md
.. include:: ./doc_neural_network.md
.. include:: ./doc_stimulus_set.md
&#34;&#34;&#34;

from .storage import *
from .fitting_manager import FittingManager
from .input_manager import *
from .output_manager import *
from .plot import *

from .networks import *
from .input_generator import *
from .statistics_helper import *</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="nn_analysis.fitting_manager" href="fitting_manager.html">nn_analysis.fitting_manager</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="nn_analysis.input_generator" href="input_generator/index.html">nn_analysis.input_generator</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="nn_analysis.input_manager" href="input_manager.html">nn_analysis.input_manager</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="nn_analysis.networks" href="networks/index.html">nn_analysis.networks</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="nn_analysis.output_manager" href="output_manager.html">nn_analysis.output_manager</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="nn_analysis.plot" href="plot.html">nn_analysis.plot</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="nn_analysis.statistics_helper" href="statistics_helper.html">nn_analysis.statistics_helper</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="nn_analysis.storage" href="storage/index.html">nn_analysis.storage</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#installing-and-configuring-nn_analysis">Installing and configuring nn_analysis</a></li>
<li><a href="#getting-started">Getting started</a><ul>
<li><a href="#getting-the-activations">Getting the activations</a></li>
<li><a href="#fitting-activations-to-a-tuning-function">Fitting activations to a tuning function</a></li>
<li><a href="#plot-the-results">Plot the results</a></li>
</ul>
</li>
<li><a href="#adding-new-neural-networks-to-the-code-analysis-system">Adding new neural networks to the code analysis system</a><ul>
<li><a href="#pytorch-models">PyTorch models</a></li>
<li><a href="#tensorflow-models">TensorFlow models</a></li>
</ul>
</li>
<li><a href="#implement-your-own-stimulus-set">Implement your own stimulus set</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="nn_analysis.fitting_manager" href="fitting_manager.html">nn_analysis.fitting_manager</a></code></li>
<li><code><a title="nn_analysis.input_generator" href="input_generator/index.html">nn_analysis.input_generator</a></code></li>
<li><code><a title="nn_analysis.input_manager" href="input_manager.html">nn_analysis.input_manager</a></code></li>
<li><code><a title="nn_analysis.networks" href="networks/index.html">nn_analysis.networks</a></code></li>
<li><code><a title="nn_analysis.output_manager" href="output_manager.html">nn_analysis.output_manager</a></code></li>
<li><code><a title="nn_analysis.plot" href="plot.html">nn_analysis.plot</a></code></li>
<li><code><a title="nn_analysis.statistics_helper" href="statistics_helper.html">nn_analysis.statistics_helper</a></code></li>
<li><code><a title="nn_analysis.storage" href="storage/index.html">nn_analysis.storage</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>