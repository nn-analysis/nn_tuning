<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>nn_analysis.fitting_manager API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>nn_analysis.fitting_manager</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from tqdm import tqdm
from typing import Union

from .storage import StorageManager, Table, TableSet


class FittingManager:
    &#34;&#34;&#34;
    Class responsible for fitting response functions to recorded activations and calculating the best fits.

    Attributes:
        storage_manager: StorageManager used to store the results from fittings.

    Args:
        storage_manager: StorageManager used to store the results from fittings.
    &#34;&#34;&#34;

    def __init__(self, storage_manager: StorageManager):
        self.storage_manager = storage_manager

    @staticmethod
    def get_identity_stim_variables(size_x, size_y) -&gt; (np.ndarray, np.ndarray):
        &#34;&#34;&#34;
        Generates the stimulus variables for when every combination of x and y is a valid position.

        Examples
        --------
        &gt;&gt;&gt; FittingManager.get_identity_stim_variables(3, 4)
        (array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]), array([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]))
        &gt;&gt;&gt; FittingManager.get_identity_stim_variables(3, 2)
        (array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 1, 2]))

        Args:
            size_x: The size of x.
            size_y: The size of y.

        Returns:
            The stimulus variables for x and y.
        &#34;&#34;&#34;
        stim_x = []
        stim_y = []
        for x in range(1, size_x + 1):
            for y in range(1, size_y + 1):
                stim_x.append(x)
                stim_y.append(y)
        return np.array(stim_x), np.array(stim_y)

    def calculate_best_fits(self, results: Union[Table, TableSet, np.ndarray], candidate_function_parameters,
                            table: str = None):
        &#34;&#34;&#34;
        Use already generated results in a Table, TableSet, or np.ndarray to get the best fits from those sets.
        Saves those best_fits to the table.

        It the results are a TableSet this function will preserve the organisation of the original TableSet.

        Args:
            results: `Table`, `TableSet`, np.ndarray with results.
            candidate_function_parameters: The set with candidate function parameters
            table: Table to save the best fits to.

        Returns:
            Array with the resulting best_fits. If a table name has been provided, a TableSet with the best fits.
        &#34;&#34;&#34;
        best_predicted = np.zeros((results.shape[1], 4))
        results_ndarray = results[:]
        best_r2s = np.nanmax(results_ndarray[:], axis=0)
        for i in range(results_ndarray.shape[1]):
            best_r2 = best_r2s[i]
            best_index = np.where(results_ndarray[:, i] == best_r2)[0][0]
            best_x, best_y, best_s = candidate_function_parameters[best_index, 0], \
                                     candidate_function_parameters[best_index, 1], \
                                     candidate_function_parameters[best_index, 2]
            best_predicted[i] = best_r2, best_x, best_y, best_s
            i += 1
        if table is not None:
            if type(results) is TableSet:
                table_labels = results.recurrent_subtables
                return self.storage_manager.save_result_table_set(self.__unpack_tuple_according_to_labels(best_predicted, results),
                                                                  table, table_labels)
            else:
                return self.__save__(table, best_predicted, False, 0, dtype)
        return best_predicted

    def __unpack_tuple_according_to_labels(self, result: np.ndarray, table_set: TableSet) -&gt; tuple:
        &#34;&#34;&#34;
        Takes a TableSet and a result array to split the result array into parts fitting into the provided TableSet

        Args:
            result: np.ndarray containing items from a
            table_set: The TableSet the results will be formed to

        Returns:
            tuple of the results split to fit the TableSet&#39;s labels
        &#34;&#34;&#34;
        # Keep track of the ncols so far to determine which part of the results needs to be selected
        ncols_so_far = 0
        results = []
        i = 0
        # Go through the tables and subtables recursively
        for label in table_set.subtables:
            subtable = table_set.get_subtable(label)
            # Select the results for this subpart
            results_selection = result[:, ncols_so_far:ncols_so_far+subtable.ncols]
            # Either recursively enter the subtableset or add the results selection directly
            if type(subtable) is TableSet:
                results.append(self.__unpack_tuple_according_to_labels(results_selection,
                                                                       subtable))
            else:
                results.append(results_selection)
            # Update the counters
            ncols_so_far += subtable.ncols
            i += 1
        # Return the results as a tuple
        return tuple(results)

    def fit_response_function_on_table_set(self, responses: TableSet, table_set: str, stim_x: np.ndarray, stim_y: np.ndarray,
                                           candidate_function_parameters: np.ndarray,
                                           prediction_function: str = &#34;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#34;,
                                           stimulus: np.ndarray = None, parallel: bool = True, verbose: bool = False,
                                           dtype: np.dtype = None, split_calculation: bool = True):
        &#34;&#34;&#34;
        Creates a new TableSet based on the input TableSet.
        Uses the fit response function to calculate the goodness of fit for all recorded nodes in the responses TableSet.
        This can be done in a, less computationally intensive, way by setting split_calculation to True.
        Then the program will go through the activations one subtable (not subtableset) of the responses TableSet at the time.

        Besides that the function has the necessary parameters for the `fit_response_function`.
        The `fit_response_function` is a function that uses a prediction function to generate predictions for the activations of neurons for all the
        candidate function parameters described in the `candidate_function_parameters` variable.

        By default, the prediction_function is a gaussian function. Another example of a prediction functions could be &#39;stim_x**x&#39;.
        At the point of executing the prediction function stim_x, stim_y, x, y, and s are the available parameters.

        The predictions are compared to the recorded responses to determine a goodness of fit.

        See Also
        --------
        fit_response_function : The function this function uses. This function&#39;s documentation also contains some examples of input.

        Args:
            responses: Recorded activations in a TableSet.
            table_set: The name of the TableSet to save the results to.
            stim_x: The stim_x variable contains an array with, for every row in the responses, what x variables were activated at that point.
            stim_y: The stim_y variable contains an array with, for every row in the responses, what y variables were activated at that point.
            candidate_function_parameters: A numpy array with, at each row, three variables for x, y, and sigma that will be evaluated by the function.
            prediction_function: The function that will generate the prediction. by default this is a simple gaussian function.
            stimulus (optional): The stimulus variable is an np.ndarray with, at each row, an array with the list of stimuli that were activated at that point.
            parallel (optional, default=True): Boolean indicating whether the algorithm should run parallel. Parallel processing makes the algorithm a lot faster.
            verbose (optional, default=False): Boolean indicating whether the function prints progress to the console.
            dtype (optional): The data type to store the data in when storing the data in a table
            split_calculation (optional, default=True): Splits the task into parts to avoid overloading the memory or the CPU.

        Returns:
            A TableSet with the goodness of fits for all nodes in the responses table. The TableSet has the same layout as the original one.
        &#34;&#34;&#34;
        # Create a new Table with the shape of the original TableSet
            # Create tuple of Nones from the original TableSet
        def tuple_of_nones_from_original_table_set(labels: dict) -&gt; tuple:
            result = []
            for label in labels.items():
                if type(label[1]) is dict:
                    result.append(tuple_of_nones_from_original_table_set(label[1]))
                else:
                    result.append(None)
            return tuple(result)
        new_table_initialisation_data = tuple_of_nones_from_original_table_set(responses.recurrent_subtables)
            # Get the original ncols and nrows variables.
        ncols = responses.ncols_tuple
        nrows = candidate_function_parameters.shape[0]
            # Create the TableSet, checking if another one already exists with that name
        new_table_set = TableSet(table_set, self.storage_manager.database)
        if new_table_set.initialised:
            raise ValueError(&#39;A TableSet with this name already exists! Delete it or choose another name!&#39;)
        print(&#39;Initialising TableSet&#39;)
        step = 500
        for row in tqdm(range(0, nrows, step)):
            new_nrows = step
            if row * step + step &gt; nrows:
                new_nrows = nrows - row * step
            new_table_set = self.storage_manager.save_result_table_set(new_table_initialisation_data, table_set,
                                                                       responses.recurrent_subtables,
                                                                       new_nrows, ncols, append_rows=True)

        # Run the fit response function for each subtable recursively when using splitting
        def recursively_run_response_function_by_splitting(responses_in_function: TableSet, parent: str = None):
            if parent is not None:
                new_parent = f&#39;{parent} &gt; {responses_in_function.name}&#39;
            else:
                new_parent = responses_in_function.name
            # Keep track of starting column to update the TableSet
            col_start = 0
            for subtable in responses_in_function.subtables:
                subtable_instance = responses_in_function.get_subtable(subtable)
                if type(subtable_instance) is TableSet:  # Use this function recursively
                    recursively_run_response_function_by_splitting(subtable_instance, parent=new_parent)
                else:  # If node --&gt; Run the fit
                    # Print the name of the table &#34;parent &gt; child&#34;
                    print(f&#39;{new_parent} &gt; {subtable_instance.name}&#39;)
                    # Run the fit_response_function
                    results = self.fit_response_function(subtable_instance[:].T, stim_x, stim_y,
                                                         candidate_function_parameters, prediction_function,
                                                         stimulus=stimulus, parallel=parallel, verbose=verbose,
                                                         dtype=dtype)
                    # Save the result of each of those things to the table
                    self.storage_manager.save_result_table_set((results,), table_set, responses.recurrent_subtables,
                                                               col_start=col_start)
                col_start += subtable_instance.ncols

        def run_response_function_all_at_once(responses_in_function: TableSet):
            results = self.fit_response_function(responses_in_function[:].T, stim_x, stim_y,
                                                 candidate_function_parameters, prediction_function,
                                                 stimulus=stimulus, parallel=parallel, verbose=verbose,
                                                 dtype=dtype)
            self.storage_manager.save_result_table_set((results,), table_set, responses.recurrent_subtables,
                                                       col_start=0)
        print(&#39;Running calculations&#39;)
        if split_calculation:
            recursively_run_response_function_by_splitting(responses)
        else:
            run_response_function_all_at_once(responses)
        return new_table_set

    @staticmethod
    def fit_response_function(responses: np.ndarray, stim_x: np.ndarray, stim_y: np.ndarray,
                              candidate_function_parameters: np.ndarray,
                              prediction_function: str = &#34;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#34;,
                              stimulus: np.ndarray = None,
                              parallel: bool = True, verbose: bool = False,
                              dtype: np.dtype = None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Function that uses a prediction function to generate predictions for the activations of neurons for all the
        candidate function parameters described in the `candidate_function_parameters` variable.

        By default, the prediction_function is a gaussian function. Another example of a prediction functions could be &#39;stim_x**x&#39;.
        At the point of executing the prediction function stim_x, stim_y, x, y, and s are the available parameters.

        The predictions are compared to the recorded responses to determine a goodness of fit.

        Examples
        --------
        &gt;&gt;&gt; FittingManager.fit_response_function(np.array([[1,2,3],[1,2,3],[1,2,3]]),
        &gt;&gt;&gt;                                      np.array([1,2,3,1,2,3]), np.array([1,2,3,1,2,3]),
        &gt;&gt;&gt;                                      np.array([[1,1,1], [1,1,2], [1,1,3], [1,2,1]])
        &gt;&gt;&gt;                                      &#39;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#39;)
        array([0.35, 0.44, 0.99])
        &gt;&gt;&gt; FittingManager.fit_response_function(np.array([[1,2,3],[1,2,3],[1,2,3]]),
        &gt;&gt;&gt;                                      np.array([1,2,3,1,2,3]), np.array([0,0,0,0,0,0]),
        &gt;&gt;&gt;                                      np.array([[1,0,1], [1,0,2], [1,1,3], [2,0,1], ...]),
        &gt;&gt;&gt;                                      &#39;np.exp(((stim_x - x) ** 2) / (-2 * s ** 2))&#39;)
        array([0.35, 0.44, 0.24])

        Args:
            responses: Recorded activations
            stim_x: The stim_x variable contains an array with, for every row in the responses, what x variables were activated at that point.
            stim_y: The stim_y variable contains an array with, for every row in the responses, what y variables were activated at that point.
            candidate_function_parameters: A numpy array with, at each row, three variables for x, y, and sigma that will be evaluated by the function.
            prediction_function: The function that will generate the prediction. by default this is a simple gaussian function.
            stimulus (optional): The stimulus variable is an np.ndarray with, at each row, an array with the list of stimuli that were activated at that point.
            parallel (optional, default=True): Boolean indicating whether the algorithm should run parallel. Parallel processing makes the algorithm a lot faster.
            verbose (optional, default=False): Boolean indicating whether the function prints progress to the console.
            dtype (optional): The data type to store the data in when storing the data in a table

        Returns:
           np.ndarray containing the goodness of fits.
        &#34;&#34;&#34;

        # If the stimulus is None, assume that each feature was shown once and one at the time represented by an identity matrix
        if stimulus is None:
            stimulus = np.eye(len(stim_x))

        var_resp = None
        o = None
        if parallel:
            var_resp = np.var(responses, axis=1)
            o = np.ones((responses.shape[1], 1))

        responses_T = responses.T

        goodness_of_fits = np.zeros((candidate_function_parameters.shape[0], responses.shape[0]), dtype=dtype)
        for row in tqdm(range(0, candidate_function_parameters.shape[0]), disable=(not verbose), leave=False):
            x, y, s = candidate_function_parameters[row]
            evaluated_prediction_function = eval(prediction_function)
            prediction = (stimulus @ evaluated_prediction_function)[..., np.newaxis]
            if parallel:
                _x = np.concatenate((prediction, o), axis=1)
                scale = np.linalg.pinv(_x) @ responses_T
                variance_unexplained = np.var(responses_T - _x @ scale, axis=0)
                goodness_of_fit = 1 - (variance_unexplained / var_resp)  # This is the inverted portion of the variance that is unexplained
                goodness_of_fit[np.isnan(goodness_of_fit)] = 0
                goodness_of_fit[goodness_of_fit == -np.inf] = 0
                goodness_of_fits[row] = goodness_of_fit
            else:
                for response in range(0, responses.shape[0]):
                    goodness_of_fit = np.corrcoef(prediction.reshape(-1), responses[response])[0, 1]
                    goodness_of_fits[row, response] = goodness_of_fit
        return goodness_of_fits

    def __save__(self, table: str, results: np.ndarray, override: bool,
                 col_start: int, dtype: np.dtype = None):
        &#34;&#34;&#34;
        Saves the results to a TableSet.

        Args:
            table: Name of the TableSet.
            results: np.ndarray of the results.
            override: If true, overrides the existing Table/TableSet if it exists
            col_start: Position of the data
            dtype: Data type to store the data in
        &#34;&#34;&#34;
        if table is None:
            return
        if override:
            self.storage_manager.remove_table(table)
        return self.storage_manager.save_result_table_set((results.astype(dtype),), table, {table: table}, col_start=col_start)

    @staticmethod
    def generate_fake_responses(variables, stim_x, stim_y, stimulus) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Generates fake responses for the fitting test.

        Examples
        ------------
        &gt;&gt;&gt; FittingManager.generate_fake_responses([(1,1,2), (3,2,1)], *FittingManager.get_identity_stim_variables(2,3), np.array([[0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0]]))
        array([[1.48902756, 1.60653066],
               [0.44996444, 0.16417]])
        &gt;&gt;&gt; FittingManager.generate_fake_responses([(2,3,1), (1,2,1)], *FittingManager.get_identity_stim_variables(2,3), np.array([[0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0]]))
        array([[0.74186594, 0.68861566],
               [0.9744101 , 1.21306132]])

        Args:
            variables: list of tuples containing the known variables
            stim_x: Stim x of the fake responses.
            stim_y: Stim y of the fake responses.
            stimulus: The stimulus of the fake responses.

        Returns:
            np.ndarray of fake responses.
        &#34;&#34;&#34;
        nd_list = list()
        for required_x, required_y, _required_s in variables:
            g = np.exp(((stim_x - required_x) ** 2 + (stim_y - required_y) ** 2) / (-2 * _required_s ** 2))
            pred = (stimulus @ g)
            nd_list.append(pred)
        return np.array(nd_list)

    def test_response_fitting(self, variables_to_discover, stimulus, stim_x, stim_y,
                              candidate_function_parameters, parallel=False,
                              verbose=False):
        &#34;&#34;&#34;
        Tests the response fitting using known function parameters by generating fake responses, fitting parameters,
        and comparing the best fitted parameter with the known parameter.

        Args:
            variables_to_discover: list of tuples containing the known variables
            stimulus: The stimulus of the fake responses.
            stim_x: Stim x of the fake responses.
            stim_y: Stim y of the fake responses.
            candidate_function_parameters: The candidate parameters.
            parallel: Whether the function should use the parallel algorithm
            verbose: Whether the function should print progress to the command line.

        Returns:
            np.ndarray with the predictions
        &#34;&#34;&#34;
        generated_responses = self.generate_fake_responses(variables_to_discover, stim_x, stim_y, stimulus)
        p, result = self.fit_response_function(generated_responses, stim_x, stim_y, candidate_function_parameters,
                                               parallel=parallel, verbose=verbose)
        predicted = self.calculate_best_fits(result, p)
        return predicted[:, 1:]

    @staticmethod
    def linearise_sigma(log_sigma, x):
        &#34;&#34;&#34;
        Calculates a linear full width half maximum for a sigma variable.

        Examples
        -----------
        &gt;&gt;&gt; FittingManager.linearise_sigma(0.01, 3)
        0.07064623359911781
        &gt;&gt;&gt; FittingManager.linearise_sigma(0.2, 5)
        2.3766436233783077

        Args:
            log_sigma: The sigma value in log space.
            x: The corresponding variable

        Returns:

        &#34;&#34;&#34;
        log_x = np.log(x)
        fwhm_log = log_sigma * (2 * np.sqrt(2 * np.log(2)))
        fwhm_lin = np.exp(log_x + fwhm_log / 2) - np.exp(log_x - fwhm_log / 2)
        return fwhm_lin

    @staticmethod
    def init_parameter_set(step: (float, float, float), par_max: (int, int, int), par_min: (int, int, int),
                           linearise_s: bool = False,
                           log: bool = False):
        &#34;&#34;&#34;
        Initialises the candidate function parameters by using the step and shape of the candidate parameters.
        Can linearise the sigma variable and move parameters into log space.

        Examples
        ---------
        &gt;&gt;&gt; FittingManager.init_parameter_set((1.,1.,0.1), (3.,3.,0.3), (1,1,0.1), False, False)
        array([[1. , 1. , 0.1],
               [1. , 1. , 0.2],
               [1. , 2. , 0.1],
               [1. , 2. , 0.2],
               [2. , 1. , 0.1],
               [2. , 1. , 0.2],
               [2. , 2. , 0.1],
               [2. , 2. , 0.2]], dtype=float32)

        Args:
            step: (float, float, float) The step sizes of the parameters.
            par_max: (int, int, int) The maximum of the parameters.
            par_min: (int, int, int) The minimum of the parameters.
            linearise_s: (bool) If true, the sigmas will get linearised.
            log: (bool) If true, the first two parameters are moved into log space.

        Returns:
            np.ndarray with at each row a set of parameter function parameters
        &#34;&#34;&#34;
        i = 0
        p = np.zeros(((np.arange(par_min[0], par_max[0], step[0]).size *
                       np.arange(par_min[1], par_max[1], step[1]).size) *
                       np.arange(par_min[2], par_max[2], step[2]).size, 3), dtype=np.float32)
        for x in np.arange(par_min[0], par_max[0], step[0]):
            for y in np.arange(par_min[1], par_max[1], step[1]):
                for s in np.arange(par_min[2], par_max[2], step[2]):
                    if log:
                        p[i] = np.array([np.log(x), np.log(y), s])
                    else:
                        p[i] = np.array([x, y, s])
                    i += 1
        if linearise_s:
            p[:, 2] = FittingManager.linearise_sigma(p[:, 2], p[:, 0])
        return p</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="nn_analysis.fitting_manager.FittingManager"><code class="flex name class">
<span>class <span class="ident">FittingManager</span></span>
<span>(</span><span>storage_manager:Â <a title="nn_analysis.storage.storage_manager.StorageManager" href="storage/storage_manager.html#nn_analysis.storage.storage_manager.StorageManager">StorageManager</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Class responsible for fitting response functions to recorded activations and calculating the best fits.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>storage_manager</code></strong></dt>
<dd>StorageManager used to store the results from fittings.</dd>
</dl>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>storage_manager</code></strong></dt>
<dd>StorageManager used to store the results from fittings.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FittingManager:
    &#34;&#34;&#34;
    Class responsible for fitting response functions to recorded activations and calculating the best fits.

    Attributes:
        storage_manager: StorageManager used to store the results from fittings.

    Args:
        storage_manager: StorageManager used to store the results from fittings.
    &#34;&#34;&#34;

    def __init__(self, storage_manager: StorageManager):
        self.storage_manager = storage_manager

    @staticmethod
    def get_identity_stim_variables(size_x, size_y) -&gt; (np.ndarray, np.ndarray):
        &#34;&#34;&#34;
        Generates the stimulus variables for when every combination of x and y is a valid position.

        Examples
        --------
        &gt;&gt;&gt; FittingManager.get_identity_stim_variables(3, 4)
        (array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]), array([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]))
        &gt;&gt;&gt; FittingManager.get_identity_stim_variables(3, 2)
        (array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 1, 2]))

        Args:
            size_x: The size of x.
            size_y: The size of y.

        Returns:
            The stimulus variables for x and y.
        &#34;&#34;&#34;
        stim_x = []
        stim_y = []
        for x in range(1, size_x + 1):
            for y in range(1, size_y + 1):
                stim_x.append(x)
                stim_y.append(y)
        return np.array(stim_x), np.array(stim_y)

    def calculate_best_fits(self, results: Union[Table, TableSet, np.ndarray], candidate_function_parameters,
                            table: str = None):
        &#34;&#34;&#34;
        Use already generated results in a Table, TableSet, or np.ndarray to get the best fits from those sets.
        Saves those best_fits to the table.

        It the results are a TableSet this function will preserve the organisation of the original TableSet.

        Args:
            results: `Table`, `TableSet`, np.ndarray with results.
            candidate_function_parameters: The set with candidate function parameters
            table: Table to save the best fits to.

        Returns:
            Array with the resulting best_fits. If a table name has been provided, a TableSet with the best fits.
        &#34;&#34;&#34;
        best_predicted = np.zeros((results.shape[1], 4))
        results_ndarray = results[:]
        best_r2s = np.nanmax(results_ndarray[:], axis=0)
        for i in range(results_ndarray.shape[1]):
            best_r2 = best_r2s[i]
            best_index = np.where(results_ndarray[:, i] == best_r2)[0][0]
            best_x, best_y, best_s = candidate_function_parameters[best_index, 0], \
                                     candidate_function_parameters[best_index, 1], \
                                     candidate_function_parameters[best_index, 2]
            best_predicted[i] = best_r2, best_x, best_y, best_s
            i += 1
        if table is not None:
            if type(results) is TableSet:
                table_labels = results.recurrent_subtables
                return self.storage_manager.save_result_table_set(self.__unpack_tuple_according_to_labels(best_predicted, results),
                                                                  table, table_labels)
            else:
                return self.__save__(table, best_predicted, False, 0, dtype)
        return best_predicted

    def __unpack_tuple_according_to_labels(self, result: np.ndarray, table_set: TableSet) -&gt; tuple:
        &#34;&#34;&#34;
        Takes a TableSet and a result array to split the result array into parts fitting into the provided TableSet

        Args:
            result: np.ndarray containing items from a
            table_set: The TableSet the results will be formed to

        Returns:
            tuple of the results split to fit the TableSet&#39;s labels
        &#34;&#34;&#34;
        # Keep track of the ncols so far to determine which part of the results needs to be selected
        ncols_so_far = 0
        results = []
        i = 0
        # Go through the tables and subtables recursively
        for label in table_set.subtables:
            subtable = table_set.get_subtable(label)
            # Select the results for this subpart
            results_selection = result[:, ncols_so_far:ncols_so_far+subtable.ncols]
            # Either recursively enter the subtableset or add the results selection directly
            if type(subtable) is TableSet:
                results.append(self.__unpack_tuple_according_to_labels(results_selection,
                                                                       subtable))
            else:
                results.append(results_selection)
            # Update the counters
            ncols_so_far += subtable.ncols
            i += 1
        # Return the results as a tuple
        return tuple(results)

    def fit_response_function_on_table_set(self, responses: TableSet, table_set: str, stim_x: np.ndarray, stim_y: np.ndarray,
                                           candidate_function_parameters: np.ndarray,
                                           prediction_function: str = &#34;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#34;,
                                           stimulus: np.ndarray = None, parallel: bool = True, verbose: bool = False,
                                           dtype: np.dtype = None, split_calculation: bool = True):
        &#34;&#34;&#34;
        Creates a new TableSet based on the input TableSet.
        Uses the fit response function to calculate the goodness of fit for all recorded nodes in the responses TableSet.
        This can be done in a, less computationally intensive, way by setting split_calculation to True.
        Then the program will go through the activations one subtable (not subtableset) of the responses TableSet at the time.

        Besides that the function has the necessary parameters for the `fit_response_function`.
        The `fit_response_function` is a function that uses a prediction function to generate predictions for the activations of neurons for all the
        candidate function parameters described in the `candidate_function_parameters` variable.

        By default, the prediction_function is a gaussian function. Another example of a prediction functions could be &#39;stim_x**x&#39;.
        At the point of executing the prediction function stim_x, stim_y, x, y, and s are the available parameters.

        The predictions are compared to the recorded responses to determine a goodness of fit.

        See Also
        --------
        fit_response_function : The function this function uses. This function&#39;s documentation also contains some examples of input.

        Args:
            responses: Recorded activations in a TableSet.
            table_set: The name of the TableSet to save the results to.
            stim_x: The stim_x variable contains an array with, for every row in the responses, what x variables were activated at that point.
            stim_y: The stim_y variable contains an array with, for every row in the responses, what y variables were activated at that point.
            candidate_function_parameters: A numpy array with, at each row, three variables for x, y, and sigma that will be evaluated by the function.
            prediction_function: The function that will generate the prediction. by default this is a simple gaussian function.
            stimulus (optional): The stimulus variable is an np.ndarray with, at each row, an array with the list of stimuli that were activated at that point.
            parallel (optional, default=True): Boolean indicating whether the algorithm should run parallel. Parallel processing makes the algorithm a lot faster.
            verbose (optional, default=False): Boolean indicating whether the function prints progress to the console.
            dtype (optional): The data type to store the data in when storing the data in a table
            split_calculation (optional, default=True): Splits the task into parts to avoid overloading the memory or the CPU.

        Returns:
            A TableSet with the goodness of fits for all nodes in the responses table. The TableSet has the same layout as the original one.
        &#34;&#34;&#34;
        # Create a new Table with the shape of the original TableSet
            # Create tuple of Nones from the original TableSet
        def tuple_of_nones_from_original_table_set(labels: dict) -&gt; tuple:
            result = []
            for label in labels.items():
                if type(label[1]) is dict:
                    result.append(tuple_of_nones_from_original_table_set(label[1]))
                else:
                    result.append(None)
            return tuple(result)
        new_table_initialisation_data = tuple_of_nones_from_original_table_set(responses.recurrent_subtables)
            # Get the original ncols and nrows variables.
        ncols = responses.ncols_tuple
        nrows = candidate_function_parameters.shape[0]
            # Create the TableSet, checking if another one already exists with that name
        new_table_set = TableSet(table_set, self.storage_manager.database)
        if new_table_set.initialised:
            raise ValueError(&#39;A TableSet with this name already exists! Delete it or choose another name!&#39;)
        print(&#39;Initialising TableSet&#39;)
        step = 500
        for row in tqdm(range(0, nrows, step)):
            new_nrows = step
            if row * step + step &gt; nrows:
                new_nrows = nrows - row * step
            new_table_set = self.storage_manager.save_result_table_set(new_table_initialisation_data, table_set,
                                                                       responses.recurrent_subtables,
                                                                       new_nrows, ncols, append_rows=True)

        # Run the fit response function for each subtable recursively when using splitting
        def recursively_run_response_function_by_splitting(responses_in_function: TableSet, parent: str = None):
            if parent is not None:
                new_parent = f&#39;{parent} &gt; {responses_in_function.name}&#39;
            else:
                new_parent = responses_in_function.name
            # Keep track of starting column to update the TableSet
            col_start = 0
            for subtable in responses_in_function.subtables:
                subtable_instance = responses_in_function.get_subtable(subtable)
                if type(subtable_instance) is TableSet:  # Use this function recursively
                    recursively_run_response_function_by_splitting(subtable_instance, parent=new_parent)
                else:  # If node --&gt; Run the fit
                    # Print the name of the table &#34;parent &gt; child&#34;
                    print(f&#39;{new_parent} &gt; {subtable_instance.name}&#39;)
                    # Run the fit_response_function
                    results = self.fit_response_function(subtable_instance[:].T, stim_x, stim_y,
                                                         candidate_function_parameters, prediction_function,
                                                         stimulus=stimulus, parallel=parallel, verbose=verbose,
                                                         dtype=dtype)
                    # Save the result of each of those things to the table
                    self.storage_manager.save_result_table_set((results,), table_set, responses.recurrent_subtables,
                                                               col_start=col_start)
                col_start += subtable_instance.ncols

        def run_response_function_all_at_once(responses_in_function: TableSet):
            results = self.fit_response_function(responses_in_function[:].T, stim_x, stim_y,
                                                 candidate_function_parameters, prediction_function,
                                                 stimulus=stimulus, parallel=parallel, verbose=verbose,
                                                 dtype=dtype)
            self.storage_manager.save_result_table_set((results,), table_set, responses.recurrent_subtables,
                                                       col_start=0)
        print(&#39;Running calculations&#39;)
        if split_calculation:
            recursively_run_response_function_by_splitting(responses)
        else:
            run_response_function_all_at_once(responses)
        return new_table_set

    @staticmethod
    def fit_response_function(responses: np.ndarray, stim_x: np.ndarray, stim_y: np.ndarray,
                              candidate_function_parameters: np.ndarray,
                              prediction_function: str = &#34;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#34;,
                              stimulus: np.ndarray = None,
                              parallel: bool = True, verbose: bool = False,
                              dtype: np.dtype = None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Function that uses a prediction function to generate predictions for the activations of neurons for all the
        candidate function parameters described in the `candidate_function_parameters` variable.

        By default, the prediction_function is a gaussian function. Another example of a prediction functions could be &#39;stim_x**x&#39;.
        At the point of executing the prediction function stim_x, stim_y, x, y, and s are the available parameters.

        The predictions are compared to the recorded responses to determine a goodness of fit.

        Examples
        --------
        &gt;&gt;&gt; FittingManager.fit_response_function(np.array([[1,2,3],[1,2,3],[1,2,3]]),
        &gt;&gt;&gt;                                      np.array([1,2,3,1,2,3]), np.array([1,2,3,1,2,3]),
        &gt;&gt;&gt;                                      np.array([[1,1,1], [1,1,2], [1,1,3], [1,2,1]])
        &gt;&gt;&gt;                                      &#39;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#39;)
        array([0.35, 0.44, 0.99])
        &gt;&gt;&gt; FittingManager.fit_response_function(np.array([[1,2,3],[1,2,3],[1,2,3]]),
        &gt;&gt;&gt;                                      np.array([1,2,3,1,2,3]), np.array([0,0,0,0,0,0]),
        &gt;&gt;&gt;                                      np.array([[1,0,1], [1,0,2], [1,1,3], [2,0,1], ...]),
        &gt;&gt;&gt;                                      &#39;np.exp(((stim_x - x) ** 2) / (-2 * s ** 2))&#39;)
        array([0.35, 0.44, 0.24])

        Args:
            responses: Recorded activations
            stim_x: The stim_x variable contains an array with, for every row in the responses, what x variables were activated at that point.
            stim_y: The stim_y variable contains an array with, for every row in the responses, what y variables were activated at that point.
            candidate_function_parameters: A numpy array with, at each row, three variables for x, y, and sigma that will be evaluated by the function.
            prediction_function: The function that will generate the prediction. by default this is a simple gaussian function.
            stimulus (optional): The stimulus variable is an np.ndarray with, at each row, an array with the list of stimuli that were activated at that point.
            parallel (optional, default=True): Boolean indicating whether the algorithm should run parallel. Parallel processing makes the algorithm a lot faster.
            verbose (optional, default=False): Boolean indicating whether the function prints progress to the console.
            dtype (optional): The data type to store the data in when storing the data in a table

        Returns:
           np.ndarray containing the goodness of fits.
        &#34;&#34;&#34;

        # If the stimulus is None, assume that each feature was shown once and one at the time represented by an identity matrix
        if stimulus is None:
            stimulus = np.eye(len(stim_x))

        var_resp = None
        o = None
        if parallel:
            var_resp = np.var(responses, axis=1)
            o = np.ones((responses.shape[1], 1))

        responses_T = responses.T

        goodness_of_fits = np.zeros((candidate_function_parameters.shape[0], responses.shape[0]), dtype=dtype)
        for row in tqdm(range(0, candidate_function_parameters.shape[0]), disable=(not verbose), leave=False):
            x, y, s = candidate_function_parameters[row]
            evaluated_prediction_function = eval(prediction_function)
            prediction = (stimulus @ evaluated_prediction_function)[..., np.newaxis]
            if parallel:
                _x = np.concatenate((prediction, o), axis=1)
                scale = np.linalg.pinv(_x) @ responses_T
                variance_unexplained = np.var(responses_T - _x @ scale, axis=0)
                goodness_of_fit = 1 - (variance_unexplained / var_resp)  # This is the inverted portion of the variance that is unexplained
                goodness_of_fit[np.isnan(goodness_of_fit)] = 0
                goodness_of_fit[goodness_of_fit == -np.inf] = 0
                goodness_of_fits[row] = goodness_of_fit
            else:
                for response in range(0, responses.shape[0]):
                    goodness_of_fit = np.corrcoef(prediction.reshape(-1), responses[response])[0, 1]
                    goodness_of_fits[row, response] = goodness_of_fit
        return goodness_of_fits

    def __save__(self, table: str, results: np.ndarray, override: bool,
                 col_start: int, dtype: np.dtype = None):
        &#34;&#34;&#34;
        Saves the results to a TableSet.

        Args:
            table: Name of the TableSet.
            results: np.ndarray of the results.
            override: If true, overrides the existing Table/TableSet if it exists
            col_start: Position of the data
            dtype: Data type to store the data in
        &#34;&#34;&#34;
        if table is None:
            return
        if override:
            self.storage_manager.remove_table(table)
        return self.storage_manager.save_result_table_set((results.astype(dtype),), table, {table: table}, col_start=col_start)

    @staticmethod
    def generate_fake_responses(variables, stim_x, stim_y, stimulus) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Generates fake responses for the fitting test.

        Examples
        ------------
        &gt;&gt;&gt; FittingManager.generate_fake_responses([(1,1,2), (3,2,1)], *FittingManager.get_identity_stim_variables(2,3), np.array([[0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0]]))
        array([[1.48902756, 1.60653066],
               [0.44996444, 0.16417]])
        &gt;&gt;&gt; FittingManager.generate_fake_responses([(2,3,1), (1,2,1)], *FittingManager.get_identity_stim_variables(2,3), np.array([[0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0]]))
        array([[0.74186594, 0.68861566],
               [0.9744101 , 1.21306132]])

        Args:
            variables: list of tuples containing the known variables
            stim_x: Stim x of the fake responses.
            stim_y: Stim y of the fake responses.
            stimulus: The stimulus of the fake responses.

        Returns:
            np.ndarray of fake responses.
        &#34;&#34;&#34;
        nd_list = list()
        for required_x, required_y, _required_s in variables:
            g = np.exp(((stim_x - required_x) ** 2 + (stim_y - required_y) ** 2) / (-2 * _required_s ** 2))
            pred = (stimulus @ g)
            nd_list.append(pred)
        return np.array(nd_list)

    def test_response_fitting(self, variables_to_discover, stimulus, stim_x, stim_y,
                              candidate_function_parameters, parallel=False,
                              verbose=False):
        &#34;&#34;&#34;
        Tests the response fitting using known function parameters by generating fake responses, fitting parameters,
        and comparing the best fitted parameter with the known parameter.

        Args:
            variables_to_discover: list of tuples containing the known variables
            stimulus: The stimulus of the fake responses.
            stim_x: Stim x of the fake responses.
            stim_y: Stim y of the fake responses.
            candidate_function_parameters: The candidate parameters.
            parallel: Whether the function should use the parallel algorithm
            verbose: Whether the function should print progress to the command line.

        Returns:
            np.ndarray with the predictions
        &#34;&#34;&#34;
        generated_responses = self.generate_fake_responses(variables_to_discover, stim_x, stim_y, stimulus)
        p, result = self.fit_response_function(generated_responses, stim_x, stim_y, candidate_function_parameters,
                                               parallel=parallel, verbose=verbose)
        predicted = self.calculate_best_fits(result, p)
        return predicted[:, 1:]

    @staticmethod
    def linearise_sigma(log_sigma, x):
        &#34;&#34;&#34;
        Calculates a linear full width half maximum for a sigma variable.

        Examples
        -----------
        &gt;&gt;&gt; FittingManager.linearise_sigma(0.01, 3)
        0.07064623359911781
        &gt;&gt;&gt; FittingManager.linearise_sigma(0.2, 5)
        2.3766436233783077

        Args:
            log_sigma: The sigma value in log space.
            x: The corresponding variable

        Returns:

        &#34;&#34;&#34;
        log_x = np.log(x)
        fwhm_log = log_sigma * (2 * np.sqrt(2 * np.log(2)))
        fwhm_lin = np.exp(log_x + fwhm_log / 2) - np.exp(log_x - fwhm_log / 2)
        return fwhm_lin

    @staticmethod
    def init_parameter_set(step: (float, float, float), par_max: (int, int, int), par_min: (int, int, int),
                           linearise_s: bool = False,
                           log: bool = False):
        &#34;&#34;&#34;
        Initialises the candidate function parameters by using the step and shape of the candidate parameters.
        Can linearise the sigma variable and move parameters into log space.

        Examples
        ---------
        &gt;&gt;&gt; FittingManager.init_parameter_set((1.,1.,0.1), (3.,3.,0.3), (1,1,0.1), False, False)
        array([[1. , 1. , 0.1],
               [1. , 1. , 0.2],
               [1. , 2. , 0.1],
               [1. , 2. , 0.2],
               [2. , 1. , 0.1],
               [2. , 1. , 0.2],
               [2. , 2. , 0.1],
               [2. , 2. , 0.2]], dtype=float32)

        Args:
            step: (float, float, float) The step sizes of the parameters.
            par_max: (int, int, int) The maximum of the parameters.
            par_min: (int, int, int) The minimum of the parameters.
            linearise_s: (bool) If true, the sigmas will get linearised.
            log: (bool) If true, the first two parameters are moved into log space.

        Returns:
            np.ndarray with at each row a set of parameter function parameters
        &#34;&#34;&#34;
        i = 0
        p = np.zeros(((np.arange(par_min[0], par_max[0], step[0]).size *
                       np.arange(par_min[1], par_max[1], step[1]).size) *
                       np.arange(par_min[2], par_max[2], step[2]).size, 3), dtype=np.float32)
        for x in np.arange(par_min[0], par_max[0], step[0]):
            for y in np.arange(par_min[1], par_max[1], step[1]):
                for s in np.arange(par_min[2], par_max[2], step[2]):
                    if log:
                        p[i] = np.array([np.log(x), np.log(y), s])
                    else:
                        p[i] = np.array([x, y, s])
                    i += 1
        if linearise_s:
            p[:, 2] = FittingManager.linearise_sigma(p[:, 2], p[:, 0])
        return p</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="nn_analysis.fitting_manager.FittingManager.fit_response_function"><code class="name flex">
<span>def <span class="ident">fit_response_function</span></span>(<span>responses:Â numpy.ndarray, stim_x:Â numpy.ndarray, stim_y:Â numpy.ndarray, candidate_function_parameters:Â numpy.ndarray, prediction_function:Â strÂ =Â 'np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))', stimulus:Â numpy.ndarrayÂ =Â None, parallel:Â boolÂ =Â True, verbose:Â boolÂ =Â False, dtype:Â numpy.dtypeÂ =Â None) â>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Function that uses a prediction function to generate predictions for the activations of neurons for all the
candidate function parameters described in the <code>candidate_function_parameters</code> variable.</p>
<p>By default, the prediction_function is a gaussian function. Another example of a prediction functions could be 'stim_x**x'.
At the point of executing the prediction function stim_x, stim_y, x, y, and s are the available parameters.</p>
<p>The predictions are compared to the recorded responses to determine a goodness of fit.</p>
<h2 id="examples">Examples</h2>
<pre><code class="python-repl">&gt;&gt;&gt; FittingManager.fit_response_function(np.array([[1,2,3],[1,2,3],[1,2,3]]),
&gt;&gt;&gt;                                      np.array([1,2,3,1,2,3]), np.array([1,2,3,1,2,3]),
&gt;&gt;&gt;                                      np.array([[1,1,1], [1,1,2], [1,1,3], [1,2,1]])
&gt;&gt;&gt;                                      'np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))')
array([0.35, 0.44, 0.99])
&gt;&gt;&gt; FittingManager.fit_response_function(np.array([[1,2,3],[1,2,3],[1,2,3]]),
&gt;&gt;&gt;                                      np.array([1,2,3,1,2,3]), np.array([0,0,0,0,0,0]),
&gt;&gt;&gt;                                      np.array([[1,0,1], [1,0,2], [1,1,3], [2,0,1], ...]),
&gt;&gt;&gt;                                      'np.exp(((stim_x - x) ** 2) / (-2 * s ** 2))')
array([0.35, 0.44, 0.24])
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>responses</code></strong></dt>
<dd>Recorded activations</dd>
<dt><strong><code>stim_x</code></strong></dt>
<dd>The stim_x variable contains an array with, for every row in the responses, what x variables were activated at that point.</dd>
<dt><strong><code>stim_y</code></strong></dt>
<dd>The stim_y variable contains an array with, for every row in the responses, what y variables were activated at that point.</dd>
<dt><strong><code>candidate_function_parameters</code></strong></dt>
<dd>A numpy array with, at each row, three variables for x, y, and sigma that will be evaluated by the function.</dd>
<dt><strong><code>prediction_function</code></strong></dt>
<dd>The function that will generate the prediction. by default this is a simple gaussian function.</dd>
<dt><strong><code>stimulus</code></strong> :&ensp;<code>optional</code></dt>
<dd>The stimulus variable is an np.ndarray with, at each row, an array with the list of stimuli that were activated at that point.</dd>
<dt><strong><code>parallel</code></strong> :&ensp;<code>optional</code>, default=<code>True</code></dt>
<dd>Boolean indicating whether the algorithm should run parallel. Parallel processing makes the algorithm a lot faster.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>optional</code>, default=<code>False</code></dt>
<dd>Boolean indicating whether the function prints progress to the console.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>optional</code></dt>
<dd>The data type to store the data in when storing the data in a table</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>np.ndarray containing the goodness of fits.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def fit_response_function(responses: np.ndarray, stim_x: np.ndarray, stim_y: np.ndarray,
                          candidate_function_parameters: np.ndarray,
                          prediction_function: str = &#34;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#34;,
                          stimulus: np.ndarray = None,
                          parallel: bool = True, verbose: bool = False,
                          dtype: np.dtype = None) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Function that uses a prediction function to generate predictions for the activations of neurons for all the
    candidate function parameters described in the `candidate_function_parameters` variable.

    By default, the prediction_function is a gaussian function. Another example of a prediction functions could be &#39;stim_x**x&#39;.
    At the point of executing the prediction function stim_x, stim_y, x, y, and s are the available parameters.

    The predictions are compared to the recorded responses to determine a goodness of fit.

    Examples
    --------
    &gt;&gt;&gt; FittingManager.fit_response_function(np.array([[1,2,3],[1,2,3],[1,2,3]]),
    &gt;&gt;&gt;                                      np.array([1,2,3,1,2,3]), np.array([1,2,3,1,2,3]),
    &gt;&gt;&gt;                                      np.array([[1,1,1], [1,1,2], [1,1,3], [1,2,1]])
    &gt;&gt;&gt;                                      &#39;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#39;)
    array([0.35, 0.44, 0.99])
    &gt;&gt;&gt; FittingManager.fit_response_function(np.array([[1,2,3],[1,2,3],[1,2,3]]),
    &gt;&gt;&gt;                                      np.array([1,2,3,1,2,3]), np.array([0,0,0,0,0,0]),
    &gt;&gt;&gt;                                      np.array([[1,0,1], [1,0,2], [1,1,3], [2,0,1], ...]),
    &gt;&gt;&gt;                                      &#39;np.exp(((stim_x - x) ** 2) / (-2 * s ** 2))&#39;)
    array([0.35, 0.44, 0.24])

    Args:
        responses: Recorded activations
        stim_x: The stim_x variable contains an array with, for every row in the responses, what x variables were activated at that point.
        stim_y: The stim_y variable contains an array with, for every row in the responses, what y variables were activated at that point.
        candidate_function_parameters: A numpy array with, at each row, three variables for x, y, and sigma that will be evaluated by the function.
        prediction_function: The function that will generate the prediction. by default this is a simple gaussian function.
        stimulus (optional): The stimulus variable is an np.ndarray with, at each row, an array with the list of stimuli that were activated at that point.
        parallel (optional, default=True): Boolean indicating whether the algorithm should run parallel. Parallel processing makes the algorithm a lot faster.
        verbose (optional, default=False): Boolean indicating whether the function prints progress to the console.
        dtype (optional): The data type to store the data in when storing the data in a table

    Returns:
       np.ndarray containing the goodness of fits.
    &#34;&#34;&#34;

    # If the stimulus is None, assume that each feature was shown once and one at the time represented by an identity matrix
    if stimulus is None:
        stimulus = np.eye(len(stim_x))

    var_resp = None
    o = None
    if parallel:
        var_resp = np.var(responses, axis=1)
        o = np.ones((responses.shape[1], 1))

    responses_T = responses.T

    goodness_of_fits = np.zeros((candidate_function_parameters.shape[0], responses.shape[0]), dtype=dtype)
    for row in tqdm(range(0, candidate_function_parameters.shape[0]), disable=(not verbose), leave=False):
        x, y, s = candidate_function_parameters[row]
        evaluated_prediction_function = eval(prediction_function)
        prediction = (stimulus @ evaluated_prediction_function)[..., np.newaxis]
        if parallel:
            _x = np.concatenate((prediction, o), axis=1)
            scale = np.linalg.pinv(_x) @ responses_T
            variance_unexplained = np.var(responses_T - _x @ scale, axis=0)
            goodness_of_fit = 1 - (variance_unexplained / var_resp)  # This is the inverted portion of the variance that is unexplained
            goodness_of_fit[np.isnan(goodness_of_fit)] = 0
            goodness_of_fit[goodness_of_fit == -np.inf] = 0
            goodness_of_fits[row] = goodness_of_fit
        else:
            for response in range(0, responses.shape[0]):
                goodness_of_fit = np.corrcoef(prediction.reshape(-1), responses[response])[0, 1]
                goodness_of_fits[row, response] = goodness_of_fit
    return goodness_of_fits</code></pre>
</details>
</dd>
<dt id="nn_analysis.fitting_manager.FittingManager.generate_fake_responses"><code class="name flex">
<span>def <span class="ident">generate_fake_responses</span></span>(<span>variables, stim_x, stim_y, stimulus) â>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Generates fake responses for the fitting test.</p>
<h2 id="examples">Examples</h2>
<pre><code class="python-repl">&gt;&gt;&gt; FittingManager.generate_fake_responses([(1,1,2), (3,2,1)], *FittingManager.get_identity_stim_variables(2,3), np.array([[0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0]]))
array([[1.48902756, 1.60653066],
       [0.44996444, 0.16417]])
&gt;&gt;&gt; FittingManager.generate_fake_responses([(2,3,1), (1,2,1)], *FittingManager.get_identity_stim_variables(2,3), np.array([[0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0]]))
array([[0.74186594, 0.68861566],
       [0.9744101 , 1.21306132]])
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>variables</code></strong></dt>
<dd>list of tuples containing the known variables</dd>
<dt><strong><code>stim_x</code></strong></dt>
<dd>Stim x of the fake responses.</dd>
<dt><strong><code>stim_y</code></strong></dt>
<dd>Stim y of the fake responses.</dd>
<dt><strong><code>stimulus</code></strong></dt>
<dd>The stimulus of the fake responses.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>np.ndarray of fake responses.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def generate_fake_responses(variables, stim_x, stim_y, stimulus) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Generates fake responses for the fitting test.

    Examples
    ------------
    &gt;&gt;&gt; FittingManager.generate_fake_responses([(1,1,2), (3,2,1)], *FittingManager.get_identity_stim_variables(2,3), np.array([[0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0]]))
    array([[1.48902756, 1.60653066],
           [0.44996444, 0.16417]])
    &gt;&gt;&gt; FittingManager.generate_fake_responses([(2,3,1), (1,2,1)], *FittingManager.get_identity_stim_variables(2,3), np.array([[0, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 0]]))
    array([[0.74186594, 0.68861566],
           [0.9744101 , 1.21306132]])

    Args:
        variables: list of tuples containing the known variables
        stim_x: Stim x of the fake responses.
        stim_y: Stim y of the fake responses.
        stimulus: The stimulus of the fake responses.

    Returns:
        np.ndarray of fake responses.
    &#34;&#34;&#34;
    nd_list = list()
    for required_x, required_y, _required_s in variables:
        g = np.exp(((stim_x - required_x) ** 2 + (stim_y - required_y) ** 2) / (-2 * _required_s ** 2))
        pred = (stimulus @ g)
        nd_list.append(pred)
    return np.array(nd_list)</code></pre>
</details>
</dd>
<dt id="nn_analysis.fitting_manager.FittingManager.get_identity_stim_variables"><code class="name flex">
<span>def <span class="ident">get_identity_stim_variables</span></span>(<span>size_x, size_y) â>Â (<classÂ 'numpy.ndarray'>,Â <classÂ 'numpy.ndarray'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates the stimulus variables for when every combination of x and y is a valid position.</p>
<h2 id="examples">Examples</h2>
<pre><code class="python-repl">&gt;&gt;&gt; FittingManager.get_identity_stim_variables(3, 4)
(array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]), array([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]))
&gt;&gt;&gt; FittingManager.get_identity_stim_variables(3, 2)
(array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 1, 2]))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size_x</code></strong></dt>
<dd>The size of x.</dd>
<dt><strong><code>size_y</code></strong></dt>
<dd>The size of y.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The stimulus variables for x and y.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_identity_stim_variables(size_x, size_y) -&gt; (np.ndarray, np.ndarray):
    &#34;&#34;&#34;
    Generates the stimulus variables for when every combination of x and y is a valid position.

    Examples
    --------
    &gt;&gt;&gt; FittingManager.get_identity_stim_variables(3, 4)
    (array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]), array([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]))
    &gt;&gt;&gt; FittingManager.get_identity_stim_variables(3, 2)
    (array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 1, 2]))

    Args:
        size_x: The size of x.
        size_y: The size of y.

    Returns:
        The stimulus variables for x and y.
    &#34;&#34;&#34;
    stim_x = []
    stim_y = []
    for x in range(1, size_x + 1):
        for y in range(1, size_y + 1):
            stim_x.append(x)
            stim_y.append(y)
    return np.array(stim_x), np.array(stim_y)</code></pre>
</details>
</dd>
<dt id="nn_analysis.fitting_manager.FittingManager.init_parameter_set"><code class="name flex">
<span>def <span class="ident">init_parameter_set</span></span>(<span>step:Â (<classÂ 'float'>,Â <classÂ 'float'>,Â <classÂ 'float'>), par_max:Â (<classÂ 'int'>,Â <classÂ 'int'>,Â <classÂ 'int'>), par_min:Â (<classÂ 'int'>,Â <classÂ 'int'>,Â <classÂ 'int'>), linearise_s:Â boolÂ =Â False, log:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialises the candidate function parameters by using the step and shape of the candidate parameters.
Can linearise the sigma variable and move parameters into log space.</p>
<h2 id="examples">Examples</h2>
<pre><code class="python-repl">&gt;&gt;&gt; FittingManager.init_parameter_set((1.,1.,0.1), (3.,3.,0.3), (1,1,0.1), False, False)
array([[1. , 1. , 0.1],
       [1. , 1. , 0.2],
       [1. , 2. , 0.1],
       [1. , 2. , 0.2],
       [2. , 1. , 0.1],
       [2. , 1. , 0.2],
       [2. , 2. , 0.1],
       [2. , 2. , 0.2]], dtype=float32)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>step</code></strong></dt>
<dd>(float, float, float) The step sizes of the parameters.</dd>
<dt><strong><code>par_max</code></strong></dt>
<dd>(int, int, int) The maximum of the parameters.</dd>
<dt><strong><code>par_min</code></strong></dt>
<dd>(int, int, int) The minimum of the parameters.</dd>
<dt><strong><code>linearise_s</code></strong></dt>
<dd>(bool) If true, the sigmas will get linearised.</dd>
<dt><strong><code>log</code></strong></dt>
<dd>(bool) If true, the first two parameters are moved into log space.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>np.ndarray with at each row a set of parameter function parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def init_parameter_set(step: (float, float, float), par_max: (int, int, int), par_min: (int, int, int),
                       linearise_s: bool = False,
                       log: bool = False):
    &#34;&#34;&#34;
    Initialises the candidate function parameters by using the step and shape of the candidate parameters.
    Can linearise the sigma variable and move parameters into log space.

    Examples
    ---------
    &gt;&gt;&gt; FittingManager.init_parameter_set((1.,1.,0.1), (3.,3.,0.3), (1,1,0.1), False, False)
    array([[1. , 1. , 0.1],
           [1. , 1. , 0.2],
           [1. , 2. , 0.1],
           [1. , 2. , 0.2],
           [2. , 1. , 0.1],
           [2. , 1. , 0.2],
           [2. , 2. , 0.1],
           [2. , 2. , 0.2]], dtype=float32)

    Args:
        step: (float, float, float) The step sizes of the parameters.
        par_max: (int, int, int) The maximum of the parameters.
        par_min: (int, int, int) The minimum of the parameters.
        linearise_s: (bool) If true, the sigmas will get linearised.
        log: (bool) If true, the first two parameters are moved into log space.

    Returns:
        np.ndarray with at each row a set of parameter function parameters
    &#34;&#34;&#34;
    i = 0
    p = np.zeros(((np.arange(par_min[0], par_max[0], step[0]).size *
                   np.arange(par_min[1], par_max[1], step[1]).size) *
                   np.arange(par_min[2], par_max[2], step[2]).size, 3), dtype=np.float32)
    for x in np.arange(par_min[0], par_max[0], step[0]):
        for y in np.arange(par_min[1], par_max[1], step[1]):
            for s in np.arange(par_min[2], par_max[2], step[2]):
                if log:
                    p[i] = np.array([np.log(x), np.log(y), s])
                else:
                    p[i] = np.array([x, y, s])
                i += 1
    if linearise_s:
        p[:, 2] = FittingManager.linearise_sigma(p[:, 2], p[:, 0])
    return p</code></pre>
</details>
</dd>
<dt id="nn_analysis.fitting_manager.FittingManager.linearise_sigma"><code class="name flex">
<span>def <span class="ident">linearise_sigma</span></span>(<span>log_sigma, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates a linear full width half maximum for a sigma variable.</p>
<h2 id="examples">Examples</h2>
<pre><code class="python-repl">&gt;&gt;&gt; FittingManager.linearise_sigma(0.01, 3)
0.07064623359911781
&gt;&gt;&gt; FittingManager.linearise_sigma(0.2, 5)
2.3766436233783077
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>log_sigma</code></strong></dt>
<dd>The sigma value in log space.</dd>
<dt><strong><code>x</code></strong></dt>
<dd>The corresponding variable</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def linearise_sigma(log_sigma, x):
    &#34;&#34;&#34;
    Calculates a linear full width half maximum for a sigma variable.

    Examples
    -----------
    &gt;&gt;&gt; FittingManager.linearise_sigma(0.01, 3)
    0.07064623359911781
    &gt;&gt;&gt; FittingManager.linearise_sigma(0.2, 5)
    2.3766436233783077

    Args:
        log_sigma: The sigma value in log space.
        x: The corresponding variable

    Returns:

    &#34;&#34;&#34;
    log_x = np.log(x)
    fwhm_log = log_sigma * (2 * np.sqrt(2 * np.log(2)))
    fwhm_lin = np.exp(log_x + fwhm_log / 2) - np.exp(log_x - fwhm_log / 2)
    return fwhm_lin</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="nn_analysis.fitting_manager.FittingManager.calculate_best_fits"><code class="name flex">
<span>def <span class="ident">calculate_best_fits</span></span>(<span>self, results:Â Union[<a title="nn_analysis.storage.table.Table" href="storage/table.html#nn_analysis.storage.table.Table">Table</a>,Â <a title="nn_analysis.storage.table_set.TableSet" href="storage/table_set.html#nn_analysis.storage.table_set.TableSet">TableSet</a>,Â numpy.ndarray], candidate_function_parameters, table:Â strÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Use already generated results in a Table, TableSet, or np.ndarray to get the best fits from those sets.
Saves those best_fits to the table.</p>
<p>It the results are a TableSet this function will preserve the organisation of the original TableSet.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>results</code></strong></dt>
<dd><code>Table</code>, <code>TableSet</code>, np.ndarray with results.</dd>
<dt><strong><code>candidate_function_parameters</code></strong></dt>
<dd>The set with candidate function parameters</dd>
<dt><strong><code>table</code></strong></dt>
<dd>Table to save the best fits to.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Array with the resulting best_fits. If a table name has been provided, a TableSet with the best fits.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_best_fits(self, results: Union[Table, TableSet, np.ndarray], candidate_function_parameters,
                        table: str = None):
    &#34;&#34;&#34;
    Use already generated results in a Table, TableSet, or np.ndarray to get the best fits from those sets.
    Saves those best_fits to the table.

    It the results are a TableSet this function will preserve the organisation of the original TableSet.

    Args:
        results: `Table`, `TableSet`, np.ndarray with results.
        candidate_function_parameters: The set with candidate function parameters
        table: Table to save the best fits to.

    Returns:
        Array with the resulting best_fits. If a table name has been provided, a TableSet with the best fits.
    &#34;&#34;&#34;
    best_predicted = np.zeros((results.shape[1], 4))
    results_ndarray = results[:]
    best_r2s = np.nanmax(results_ndarray[:], axis=0)
    for i in range(results_ndarray.shape[1]):
        best_r2 = best_r2s[i]
        best_index = np.where(results_ndarray[:, i] == best_r2)[0][0]
        best_x, best_y, best_s = candidate_function_parameters[best_index, 0], \
                                 candidate_function_parameters[best_index, 1], \
                                 candidate_function_parameters[best_index, 2]
        best_predicted[i] = best_r2, best_x, best_y, best_s
        i += 1
    if table is not None:
        if type(results) is TableSet:
            table_labels = results.recurrent_subtables
            return self.storage_manager.save_result_table_set(self.__unpack_tuple_according_to_labels(best_predicted, results),
                                                              table, table_labels)
        else:
            return self.__save__(table, best_predicted, False, 0, dtype)
    return best_predicted</code></pre>
</details>
</dd>
<dt id="nn_analysis.fitting_manager.FittingManager.fit_response_function_on_table_set"><code class="name flex">
<span>def <span class="ident">fit_response_function_on_table_set</span></span>(<span>self, responses:Â <a title="nn_analysis.storage.table_set.TableSet" href="storage/table_set.html#nn_analysis.storage.table_set.TableSet">TableSet</a>, table_set:Â str, stim_x:Â numpy.ndarray, stim_y:Â numpy.ndarray, candidate_function_parameters:Â numpy.ndarray, prediction_function:Â strÂ =Â 'np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))', stimulus:Â numpy.ndarrayÂ =Â None, parallel:Â boolÂ =Â True, verbose:Â boolÂ =Â False, dtype:Â numpy.dtypeÂ =Â None, split_calculation:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new TableSet based on the input TableSet.
Uses the fit response function to calculate the goodness of fit for all recorded nodes in the responses TableSet.
This can be done in a, less computationally intensive, way by setting split_calculation to True.
Then the program will go through the activations one subtable (not subtableset) of the responses TableSet at the time.</p>
<p>Besides that the function has the necessary parameters for the <code>fit_response_function</code>.
The <code>fit_response_function</code> is a function that uses a prediction function to generate predictions for the activations of neurons for all the
candidate function parameters described in the <code>candidate_function_parameters</code> variable.</p>
<p>By default, the prediction_function is a gaussian function. Another example of a prediction functions could be 'stim_x**x'.
At the point of executing the prediction function stim_x, stim_y, x, y, and s are the available parameters.</p>
<p>The predictions are compared to the recorded responses to determine a goodness of fit.</p>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>fit_response_function</code></dt>
<dd>The function this function uses. This function's documentation also contains some examples of input.</dd>
</dl>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>responses</code></strong></dt>
<dd>Recorded activations in a TableSet.</dd>
<dt><strong><code>table_set</code></strong></dt>
<dd>The name of the TableSet to save the results to.</dd>
<dt><strong><code>stim_x</code></strong></dt>
<dd>The stim_x variable contains an array with, for every row in the responses, what x variables were activated at that point.</dd>
<dt><strong><code>stim_y</code></strong></dt>
<dd>The stim_y variable contains an array with, for every row in the responses, what y variables were activated at that point.</dd>
<dt><strong><code>candidate_function_parameters</code></strong></dt>
<dd>A numpy array with, at each row, three variables for x, y, and sigma that will be evaluated by the function.</dd>
<dt><strong><code>prediction_function</code></strong></dt>
<dd>The function that will generate the prediction. by default this is a simple gaussian function.</dd>
<dt><strong><code>stimulus</code></strong> :&ensp;<code>optional</code></dt>
<dd>The stimulus variable is an np.ndarray with, at each row, an array with the list of stimuli that were activated at that point.</dd>
<dt><strong><code>parallel</code></strong> :&ensp;<code>optional</code>, default=<code>True</code></dt>
<dd>Boolean indicating whether the algorithm should run parallel. Parallel processing makes the algorithm a lot faster.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>optional</code>, default=<code>False</code></dt>
<dd>Boolean indicating whether the function prints progress to the console.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>optional</code></dt>
<dd>The data type to store the data in when storing the data in a table</dd>
<dt><strong><code>split_calculation</code></strong> :&ensp;<code>optional</code>, default=<code>True</code></dt>
<dd>Splits the task into parts to avoid overloading the memory or the CPU.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A TableSet with the goodness of fits for all nodes in the responses table. The TableSet has the same layout as the original one.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_response_function_on_table_set(self, responses: TableSet, table_set: str, stim_x: np.ndarray, stim_y: np.ndarray,
                                       candidate_function_parameters: np.ndarray,
                                       prediction_function: str = &#34;np.exp(((stim_x - x) ** 2 + (stim_y - y) ** 2) / (-2 * s ** 2))&#34;,
                                       stimulus: np.ndarray = None, parallel: bool = True, verbose: bool = False,
                                       dtype: np.dtype = None, split_calculation: bool = True):
    &#34;&#34;&#34;
    Creates a new TableSet based on the input TableSet.
    Uses the fit response function to calculate the goodness of fit for all recorded nodes in the responses TableSet.
    This can be done in a, less computationally intensive, way by setting split_calculation to True.
    Then the program will go through the activations one subtable (not subtableset) of the responses TableSet at the time.

    Besides that the function has the necessary parameters for the `fit_response_function`.
    The `fit_response_function` is a function that uses a prediction function to generate predictions for the activations of neurons for all the
    candidate function parameters described in the `candidate_function_parameters` variable.

    By default, the prediction_function is a gaussian function. Another example of a prediction functions could be &#39;stim_x**x&#39;.
    At the point of executing the prediction function stim_x, stim_y, x, y, and s are the available parameters.

    The predictions are compared to the recorded responses to determine a goodness of fit.

    See Also
    --------
    fit_response_function : The function this function uses. This function&#39;s documentation also contains some examples of input.

    Args:
        responses: Recorded activations in a TableSet.
        table_set: The name of the TableSet to save the results to.
        stim_x: The stim_x variable contains an array with, for every row in the responses, what x variables were activated at that point.
        stim_y: The stim_y variable contains an array with, for every row in the responses, what y variables were activated at that point.
        candidate_function_parameters: A numpy array with, at each row, three variables for x, y, and sigma that will be evaluated by the function.
        prediction_function: The function that will generate the prediction. by default this is a simple gaussian function.
        stimulus (optional): The stimulus variable is an np.ndarray with, at each row, an array with the list of stimuli that were activated at that point.
        parallel (optional, default=True): Boolean indicating whether the algorithm should run parallel. Parallel processing makes the algorithm a lot faster.
        verbose (optional, default=False): Boolean indicating whether the function prints progress to the console.
        dtype (optional): The data type to store the data in when storing the data in a table
        split_calculation (optional, default=True): Splits the task into parts to avoid overloading the memory or the CPU.

    Returns:
        A TableSet with the goodness of fits for all nodes in the responses table. The TableSet has the same layout as the original one.
    &#34;&#34;&#34;
    # Create a new Table with the shape of the original TableSet
        # Create tuple of Nones from the original TableSet
    def tuple_of_nones_from_original_table_set(labels: dict) -&gt; tuple:
        result = []
        for label in labels.items():
            if type(label[1]) is dict:
                result.append(tuple_of_nones_from_original_table_set(label[1]))
            else:
                result.append(None)
        return tuple(result)
    new_table_initialisation_data = tuple_of_nones_from_original_table_set(responses.recurrent_subtables)
        # Get the original ncols and nrows variables.
    ncols = responses.ncols_tuple
    nrows = candidate_function_parameters.shape[0]
        # Create the TableSet, checking if another one already exists with that name
    new_table_set = TableSet(table_set, self.storage_manager.database)
    if new_table_set.initialised:
        raise ValueError(&#39;A TableSet with this name already exists! Delete it or choose another name!&#39;)
    print(&#39;Initialising TableSet&#39;)
    step = 500
    for row in tqdm(range(0, nrows, step)):
        new_nrows = step
        if row * step + step &gt; nrows:
            new_nrows = nrows - row * step
        new_table_set = self.storage_manager.save_result_table_set(new_table_initialisation_data, table_set,
                                                                   responses.recurrent_subtables,
                                                                   new_nrows, ncols, append_rows=True)

    # Run the fit response function for each subtable recursively when using splitting
    def recursively_run_response_function_by_splitting(responses_in_function: TableSet, parent: str = None):
        if parent is not None:
            new_parent = f&#39;{parent} &gt; {responses_in_function.name}&#39;
        else:
            new_parent = responses_in_function.name
        # Keep track of starting column to update the TableSet
        col_start = 0
        for subtable in responses_in_function.subtables:
            subtable_instance = responses_in_function.get_subtable(subtable)
            if type(subtable_instance) is TableSet:  # Use this function recursively
                recursively_run_response_function_by_splitting(subtable_instance, parent=new_parent)
            else:  # If node --&gt; Run the fit
                # Print the name of the table &#34;parent &gt; child&#34;
                print(f&#39;{new_parent} &gt; {subtable_instance.name}&#39;)
                # Run the fit_response_function
                results = self.fit_response_function(subtable_instance[:].T, stim_x, stim_y,
                                                     candidate_function_parameters, prediction_function,
                                                     stimulus=stimulus, parallel=parallel, verbose=verbose,
                                                     dtype=dtype)
                # Save the result of each of those things to the table
                self.storage_manager.save_result_table_set((results,), table_set, responses.recurrent_subtables,
                                                           col_start=col_start)
            col_start += subtable_instance.ncols

    def run_response_function_all_at_once(responses_in_function: TableSet):
        results = self.fit_response_function(responses_in_function[:].T, stim_x, stim_y,
                                             candidate_function_parameters, prediction_function,
                                             stimulus=stimulus, parallel=parallel, verbose=verbose,
                                             dtype=dtype)
        self.storage_manager.save_result_table_set((results,), table_set, responses.recurrent_subtables,
                                                   col_start=0)
    print(&#39;Running calculations&#39;)
    if split_calculation:
        recursively_run_response_function_by_splitting(responses)
    else:
        run_response_function_all_at_once(responses)
    return new_table_set</code></pre>
</details>
</dd>
<dt id="nn_analysis.fitting_manager.FittingManager.test_response_fitting"><code class="name flex">
<span>def <span class="ident">test_response_fitting</span></span>(<span>self, variables_to_discover, stimulus, stim_x, stim_y, candidate_function_parameters, parallel=False, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Tests the response fitting using known function parameters by generating fake responses, fitting parameters,
and comparing the best fitted parameter with the known parameter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>variables_to_discover</code></strong></dt>
<dd>list of tuples containing the known variables</dd>
<dt><strong><code>stimulus</code></strong></dt>
<dd>The stimulus of the fake responses.</dd>
<dt><strong><code>stim_x</code></strong></dt>
<dd>Stim x of the fake responses.</dd>
<dt><strong><code>stim_y</code></strong></dt>
<dd>Stim y of the fake responses.</dd>
<dt><strong><code>candidate_function_parameters</code></strong></dt>
<dd>The candidate parameters.</dd>
<dt><strong><code>parallel</code></strong></dt>
<dd>Whether the function should use the parallel algorithm</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Whether the function should print progress to the command line.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>np.ndarray with the predictions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_response_fitting(self, variables_to_discover, stimulus, stim_x, stim_y,
                          candidate_function_parameters, parallel=False,
                          verbose=False):
    &#34;&#34;&#34;
    Tests the response fitting using known function parameters by generating fake responses, fitting parameters,
    and comparing the best fitted parameter with the known parameter.

    Args:
        variables_to_discover: list of tuples containing the known variables
        stimulus: The stimulus of the fake responses.
        stim_x: Stim x of the fake responses.
        stim_y: Stim y of the fake responses.
        candidate_function_parameters: The candidate parameters.
        parallel: Whether the function should use the parallel algorithm
        verbose: Whether the function should print progress to the command line.

    Returns:
        np.ndarray with the predictions
    &#34;&#34;&#34;
    generated_responses = self.generate_fake_responses(variables_to_discover, stim_x, stim_y, stimulus)
    p, result = self.fit_response_function(generated_responses, stim_x, stim_y, candidate_function_parameters,
                                           parallel=parallel, verbose=verbose)
    predicted = self.calculate_best_fits(result, p)
    return predicted[:, 1:]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="nn_analysis" href="index.html">nn_analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="nn_analysis.fitting_manager.FittingManager" href="#nn_analysis.fitting_manager.FittingManager">FittingManager</a></code></h4>
<ul class="">
<li><code><a title="nn_analysis.fitting_manager.FittingManager.calculate_best_fits" href="#nn_analysis.fitting_manager.FittingManager.calculate_best_fits">calculate_best_fits</a></code></li>
<li><code><a title="nn_analysis.fitting_manager.FittingManager.fit_response_function" href="#nn_analysis.fitting_manager.FittingManager.fit_response_function">fit_response_function</a></code></li>
<li><code><a title="nn_analysis.fitting_manager.FittingManager.fit_response_function_on_table_set" href="#nn_analysis.fitting_manager.FittingManager.fit_response_function_on_table_set">fit_response_function_on_table_set</a></code></li>
<li><code><a title="nn_analysis.fitting_manager.FittingManager.generate_fake_responses" href="#nn_analysis.fitting_manager.FittingManager.generate_fake_responses">generate_fake_responses</a></code></li>
<li><code><a title="nn_analysis.fitting_manager.FittingManager.get_identity_stim_variables" href="#nn_analysis.fitting_manager.FittingManager.get_identity_stim_variables">get_identity_stim_variables</a></code></li>
<li><code><a title="nn_analysis.fitting_manager.FittingManager.init_parameter_set" href="#nn_analysis.fitting_manager.FittingManager.init_parameter_set">init_parameter_set</a></code></li>
<li><code><a title="nn_analysis.fitting_manager.FittingManager.linearise_sigma" href="#nn_analysis.fitting_manager.FittingManager.linearise_sigma">linearise_sigma</a></code></li>
<li><code><a title="nn_analysis.fitting_manager.FittingManager.test_response_fitting" href="#nn_analysis.fitting_manager.FittingManager.test_response_fitting">test_response_fitting</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>